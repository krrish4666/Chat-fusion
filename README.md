# ğŸ¤– Chat Fusion: Gemini + Llama Chatbot

Chat Fusion is a **hybrid AI chatbot** that combines the power of **Google Gemini (2.5 Pro & 2.5 Flash)** with a **locally hosted Llama model (TinyLlama-1.1B-Chat)**.  
It is built with **Streamlit** and demonstrates how cloud-based AI and local inference can work together in one unified interface.

---

## ğŸ”— Repository
GitHub Repo: [Chat Fusion](https://github.com/krrish4666/Chat-fusion)

---

## âœ¨ Features
- ğŸŒ **Gemini Integration** (2.5 Pro & Flash via Google GenAI API)  
- ğŸ’» **Local Model Support** with TinyLlama (runs on CPU/GPU)  
- âš¡ **Streaming responses** for Gemini (real-time typing effect)  
- ğŸ” **Tokenizer Explorer** â€“ compare Gemini vs Llama tokenization  
- ğŸ§  **Conversation memory** with adjustable history length  
- ğŸ§¹ **Clear chat history** option for fresh starts  

---

## ğŸ› ï¸ Tech Stack
- [Streamlit](https://streamlit.io/) â€“ frontend UI framework  
- [Google GenAI SDK](https://pypi.org/project/google-genai/) â€“ Gemini integration  
- [Transformers (Hugging Face)](https://huggingface.co/docs/transformers) â€“ local Llama model  
- [PyTorch](https://pytorch.org/) â€“ deep learning backend  

---

## âš¡ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/krrish4666/Chat-fusion.git
cd Chat-fusion
```
### 2. Install Dependencies
```bash
pip install -r requirements.txt
```
### 3. Set up API Key

Export your Gemini API key as an environment variable:

Linux / macOS:
```bash
export GEMINI_API_KEY="your_api_key_here"
```
Windows PowerShell:
```bash
set GEMINI_API_KEY="your_api_key_here"
```

Alternatively, create a .env file and load it in app.py.

4. Run the App
```bash
streamlit run app.py
```

ğŸ‘¤ Author

Built with â¤ï¸ by Krishna Yadav
